{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63685476-7b54-49ab-aa48-3ea1e1513027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Filter out the specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.feature_selection._univariate_selection\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"sklearn.feature_selection._univariate_selection\")\n",
    "\n",
    "# Set professional style with a modern color palette\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "plt.rcParams['figure.titleweight'] = 'bold'\n",
    "\n",
    "# Load the YAML files with feature descriptions and response codes\n",
    "def load_feature_descriptions(file_path='ngs2020_questions.yaml'):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            questions = yaml.safe_load(file)\n",
    "        return questions\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return {}\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error parsing YAML file: {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_response_codes(file_path='ngs2020_responses.yaml'):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            responses = yaml.safe_load(file)\n",
    "        return responses\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return {}\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error parsing YAML file: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load feature descriptions and response codes\n",
    "feature_descriptions = load_feature_descriptions()\n",
    "response_codes = load_response_codes()\n",
    "\n",
    "# Function to get human-readable feature names\n",
    "def get_feature_name(feature_code):\n",
    "    return feature_descriptions.get(feature_code, feature_code)\n",
    "\n",
    "# Function to get human-readable response values\n",
    "def get_response_value(feature_code, value):\n",
    "    if feature_code in response_codes and str(value) in response_codes[feature_code]:\n",
    "        return response_codes[feature_code][str(value)]\n",
    "    return value\n",
    "\n",
    "# Function to map a series to human-readable values\n",
    "def map_series_to_readable(series, feature_code):\n",
    "    if feature_code in response_codes:\n",
    "        mapping = response_codes[feature_code]\n",
    "        return series.map(lambda x: mapping.get(str(x), x))\n",
    "    return series\n",
    "\n",
    "# Function to get readable labels for plotting\n",
    "def get_readable_labels(feature_code, values):\n",
    "    if feature_code in response_codes:\n",
    "        return [response_codes[feature_code].get(str(val), str(val)) for val in values]\n",
    "    return [str(val) for val in values]\n",
    "\n",
    "# Load the actual data\n",
    "df = pd.read_csv('ngs2020.csv')\n",
    "\n",
    "# Print available columns to help debug\n",
    "print(\"Available columns in dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Define missing value codes based on the data documentation\n",
    "missing_codes = [6, 7, 8, 9, 96, 97, 98, 99]\n",
    "\n",
    "# Create a function to visualize missing data\n",
    "def plot_missing_data(df):\n",
    "    missing = df.isin(missing_codes).mean() * 100\n",
    "    missing = missing[missing > 0]\n",
    "    missing.sort_values(inplace=True)\n",
    "    \n",
    "    # Use human-readable feature names\n",
    "    missing.index = [get_feature_name(col) for col in missing.index]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 19))\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(missing)))\n",
    "    bars = ax.barh(missing.index, missing.values, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value annotations on bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.1f}%', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel('Percentage (%)', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('Column Name', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Percentage of Missing/Special Values by Column', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Remove spines\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    \n",
    "    # Add a subtle background\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_missing_data(df)\n",
    "\n",
    "# Data cleaning - replace missing codes with NaN\n",
    "# Special handling for VISBMINP and GRADAGEP to preserve category 9 as a valid response\n",
    "preserve_codes = {'VISBMINP': [9], 'GRADAGEP': [9]}  # Codes to keep as-is for specific variables\n",
    "\n",
    "# Don't apply missing code replacement to the program column\n",
    "columns_to_clean = [col for col in df.columns if col != 'PGMCIPAP']\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    if df[col].dtype in ['int64', 'float64']:\n",
    "        if col in preserve_codes:\n",
    "            # For variables with preserved codes, only replace codes not in the preserve list\n",
    "            codes_to_replace = [c for c in missing_codes if c not in preserve_codes[col]]\n",
    "            df[col] = df[col].replace(codes_to_replace, np.nan)\n",
    "        else:\n",
    "            # For all other variables, replace all missing codes\n",
    "            df[col] = df[col].replace(missing_codes, np.nan)\n",
    "\n",
    "# For the program column, only replace the actual missing codes (96, 97, 98, 99)\n",
    "if 'PGMCIPAP' in df.columns:\n",
    "    df['PGMCIPAP'] = df['PGMCIPAP'].replace([96, 97, 98, 99], np.nan)\n",
    "\n",
    "# Employment Status Analysis\n",
    "def employment_analysis(df):\n",
    "    print(\"\\n=== Employment Status Analysis ===\\n\")\n",
    "    \n",
    "    if 'LFSTATP' not in df.columns:\n",
    "        print(\"Employment status data not available.\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure for employment status plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Color palette\n",
    "    colors = sns.color_palette(\"husl\", 8)\n",
    "    \n",
    "    # Employment status distribution\n",
    "    # Ensure we include all expected categories\n",
    "    employment_counts = df['LFSTATP'].value_counts(dropna=False)\n",
    "    employment_labels = get_readable_labels('LFSTATP', employment_counts.index)\n",
    "    \n",
    "    print(f\"{get_feature_name('LFSTATP')} Distribution:\")\n",
    "    for label, count in zip(employment_labels, employment_counts):\n",
    "        print(f\"{label}: {count}\")\n",
    "    \n",
    "    # Create pie chart for employment status\n",
    "    explode = [0.05] * len(employment_counts)  # explode slices for emphasis\n",
    "    wedges, texts, autotexts = axes[0].pie(employment_counts, labels=employment_labels, autopct='%1.1f%%',\n",
    "                                          colors=colors[:len(employment_counts)], startangle=90, explode=explode,\n",
    "                                          shadow=True, textprops={'fontsize': 10})\n",
    "    \n",
    "    # Style the pie chart\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(10)\n",
    "    \n",
    "    axes[0].set_title(f'{get_feature_name(\"LFSTATP\")} Distribution', fontweight='bold', fontsize=14, pad=20)\n",
    "    axes[0].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    \n",
    "    # Employment status by education level\n",
    "    if 'CERTLEVP' in df.columns:\n",
    "        # Create cross-tabulation\n",
    "        emp_by_edu = pd.crosstab(df['CERTLEVP'], df['LFSTATP'])\n",
    "        \n",
    "        # Convert to percentages\n",
    "        emp_by_edu_pct = emp_by_edu.div(emp_by_edu.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        # Get readable labels\n",
    "        edu_labels = get_readable_labels('CERTLEVP', emp_by_edu_pct.index)\n",
    "        emp_labels = get_readable_labels('LFSTATP', emp_by_edu_pct.columns)\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        bottom = np.zeros(len(edu_labels))\n",
    "        for i, emp_status in enumerate(emp_by_edu_pct.columns):\n",
    "            axes[1].bar(edu_labels, emp_by_edu_pct[emp_status], bottom=bottom, \n",
    "                       label=emp_labels[i], color=colors[i], alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "            bottom += emp_by_edu_pct[emp_status]\n",
    "        \n",
    "        axes[1].set_title('Employment Status by Education Level', fontweight='bold', fontsize=14, pad=20)\n",
    "        axes[1].set_ylabel('Percentage (%)', fontweight='bold', fontsize=12)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Add grid\n",
    "        axes[1].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Remove spines\n",
    "        axes[1].spines[['top', 'right']].set_visible(False)\n",
    "    else:\n",
    "        axes[1].set_visible(False)\n",
    "        print(\"Education level data (CERTLEVP) not available for employment analysis.\")\n",
    "    \n",
    "    # Add a background color to the figure\n",
    "    fig.patch.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "employment_analysis(df)\n",
    "\n",
    "# Correlation Analysis with Employment Focus\n",
    "def correlation_analysis(df):\n",
    "    print(\"\\n=== Correlation Analysis ===\\n\")\n",
    "    \n",
    "    # Select columns that might have meaningful correlations with employment\n",
    "    corr_cols = [\n",
    "        'GRADAGEP', \n",
    "        'PERSINCP', \n",
    "        'JOBINCP',  \n",
    "        'STULOANS', \n",
    "        'LFSTATP',  \n",
    "        'CERTLEVP',\n",
    "    ]\n",
    "    \n",
    "    # Add additional columns if they exist\n",
    "    optional_cols = ['LFCINDP', 'LFCOCCP', 'COV_010']\n",
    "    for col in optional_cols:\n",
    "        if col in df.columns:\n",
    "            corr_cols.append(col)\n",
    "    \n",
    "    # Filter to only include columns that exist in the dataset\n",
    "    corr_cols = [col for col in corr_cols if col in df.columns]\n",
    "    \n",
    "    if len(corr_cols) < 2:\n",
    "        print(\"Not enough columns for correlation analysis.\")\n",
    "        return\n",
    "    \n",
    "    corr_df = df[corr_cols].copy()\n",
    "    \n",
    "    # Filter out missing codes\n",
    "    for col in corr_cols:\n",
    "        corr_df = corr_df[~corr_df[col].isin(missing_codes)]\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = corr_df.corr()\n",
    "    \n",
    "    # Get human-readable labels\n",
    "    human_labels = [get_feature_name(col) for col in corr_cols]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Create heatmap with mask\n",
    "    heatmap = sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        cmap='RdBu_r',\n",
    "        center=0,\n",
    "        fmt=\".2f\",\n",
    "        square=True,\n",
    "        mask=mask,\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "        annot_kws={\"size\": 11, \"weight\": \"bold\"},\n",
    "        linewidths=0.5,\n",
    "        linecolor='white'\n",
    "    )\n",
    "    \n",
    "    # Set title\n",
    "    plt.title('Correlation Matrix of Key Variables (Employment Focus)', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Set x-axis labels with rotation\n",
    "    heatmap.set_xticklabels(human_labels, rotation=30, ha='right', fontsize=18)\n",
    "    \n",
    "    # Set y-axis labels with proper rotation and alignment\n",
    "    heatmap.set_yticklabels(human_labels, rotation=0, va='center', fontsize=18)\n",
    "    \n",
    "    # Add a background\n",
    "    plt.gca().set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "correlation_analysis(df)\n",
    "\n",
    "# Define potential features for employment prediction modeling\n",
    "potential_features = [\n",
    "    'GENDER2', 'CERTLEVP', 'PERSINCP', 'GRADAGEP',\n",
    "    'VISBMINP', 'CTZSHIPP', 'MS_P01', 'REG_INST', 'EDU_010',\n",
    "    'PGMCIPAP', 'STULOANS', 'JOBINCP'\n",
    "]\n",
    "\n",
    "# Add optional features if they exist in the dataset\n",
    "optional_features = ['LFCINDP', 'LFCOCCP', 'LMA6_11', 'COV_010', 'LMA_010', 'LMA_020', 'LMA_030']\n",
    "for feature in optional_features:\n",
    "    if feature in df.columns:\n",
    "        potential_features.append(feature)\n",
    "\n",
    "# Filter to only include columns that exist in the dataset\n",
    "potential_features = [col for col in potential_features if col in df.columns]\n",
    "\n",
    "print(f\"Using the following features for modeling: {potential_features}\")\n",
    "\n",
    "# Predictive Modeling with Feature Selection for Employment\n",
    "def predict_employment(df, features):\n",
    "    print(\"\\n=== Predictive Modeling: Employment Status Prediction ===\\n\")\n",
    "    \n",
    "    if 'LFSTATP' not in df.columns:\n",
    "        print(\"Employment status (LFSTATP) not available for modeling.\")\n",
    "        return\n",
    "    \n",
    "    # Remove job-related features that are 100% missing for unemployed individuals\n",
    "    job_features_to_remove = ['JOBINCP', 'LFCINDP', 'LFCOCCP', 'LMA6_11']\n",
    "    features = [f for f in features if f not in job_features_to_remove]\n",
    "    print(f\"Removed job-related features: {job_features_to_remove}\")\n",
    "    print(f\"Using features: {[get_feature_name(f) for f in features]}\")\n",
    "    \n",
    "    # Prepare data with potential features\n",
    "    model_df = df[features + ['LFSTATP']].copy()\n",
    "    \n",
    "    # Replace missing codes with NaN\n",
    "    for col in model_df.columns:\n",
    "        if model_df[col].dtype in ['int64', 'float64']:\n",
    "            if col in preserve_codes:\n",
    "                codes_to_replace = [c for c in missing_codes if c not in preserve_codes[col]]\n",
    "                model_df[col] = model_df[col].replace(codes_to_replace, np.nan)\n",
    "            else:\n",
    "                model_df[col] = model_df[col].replace(missing_codes, np.nan)\n",
    "    \n",
    "    # Filter for employed and unemployed only\n",
    "    model_df = model_df[model_df['LFSTATP'].isin([1, 2])]\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    model_df = model_df.dropna()\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = model_df['LFSTATP'].value_counts()\n",
    "    print(f\"Class distribution after processing: {dict(class_counts)}\")\n",
    "    \n",
    "    if len(class_counts) < 2:\n",
    "        print(\"Warning: Still only one class after removing job-related features.\")\n",
    "        print(\"This suggests other features are still causing issues.\")\n",
    "        return\n",
    "    \n",
    "    if len(model_df) < 100:\n",
    "        print(f\"Not enough data for modeling. Only {len(model_df)} samples available.\")\n",
    "        return\n",
    "    \n",
    "    # Convert categorical variables to numerical\n",
    "    le = LabelEncoder()\n",
    "    for col in features:\n",
    "        if model_df[col].dtype == 'object':\n",
    "            model_df[col] = le.fit_transform(model_df[col])\n",
    "    \n",
    "    # Split data with stratification\n",
    "    X = model_df.drop('LFSTATP', axis=1)\n",
    "    y = model_df['LFSTATP']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Remove constant features\n",
    "    selector = VarianceThreshold()\n",
    "    X_train_clean = selector.fit_transform(X_train)\n",
    "    X_test_clean = selector.transform(X_test)\n",
    "    \n",
    "    # Get the feature names after removing constant features\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    X_train = pd.DataFrame(X_train_clean, columns=selected_features)\n",
    "    X_test = pd.DataFrame(X_test_clean, columns=selected_features)\n",
    "    \n",
    "    if len(selected_features) == 0:\n",
    "        print(\"No features remaining after variance threshold. Cannot proceed with modeling.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "    print(f\"After SMOTE - Class distribution: {np.bincount(y_train_res)}\")\n",
    "    \n",
    "    # Feature Selection Methods (using resampled data)\n",
    "    # Method 1: SelectKBest with ANOVA F-value\n",
    "    print(\"1. SelectKBest Feature Selection:\")\n",
    "    k = min(5, len(selected_features))\n",
    "    selector_kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_kbest = selector_kbest.fit_transform(X_train_res, y_train_res)\n",
    "    selected_features_kbest = selected_features[selector_kbest.get_support()]\n",
    "    selected_features_kbest_desc = [get_feature_name(feat) for feat in selected_features_kbest]\n",
    "    print(f\"Selected features: {selected_features_kbest_desc}\")\n",
    "    \n",
    "    # Method 2: Recursive Feature Elimination (RFE)\n",
    "    print(\"\\n2. Recursive Feature Elimination (RFE):\")\n",
    "    estimator = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "    n_features = min(5, len(selected_features))\n",
    "    selector_rfe = RFE(estimator, n_features_to_select=n_features, step=1)\n",
    "    X_rfe = selector_rfe.fit_transform(X_train_res, y_train_res)\n",
    "    selected_features_rfe = selected_features[selector_rfe.get_support()]\n",
    "    selected_features_rfe_desc = [get_feature_name(feat) for feat in selected_features_rfe]\n",
    "    print(f\"Selected features: {selected_features_rfe_desc}\")\n",
    "    \n",
    "    # Method 3: Feature Importance from Random Forest\n",
    "    print(\"\\n3. Random Forest Feature Importance:\")\n",
    "    rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    rf.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    importance = pd.Series(rf.feature_importances_, index=selected_features)\n",
    "    importance.index = [get_feature_name(feat) for feat in importance.index]\n",
    "    importance = importance.sort_values(ascending=True)\n",
    "    \n",
    "    # Create a horizontal bar chart for feature importance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(importance)))\n",
    "    bars = plt.barh(importance.index, importance.values, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value annotations on bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Styling\n",
    "    plt.xlabel('Importance Score', fontweight='bold', fontsize=12)\n",
    "    plt.ylabel('Features', fontweight='bold', fontsize=12)\n",
    "    plt.title('Feature Importance for Employment Status Prediction', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Remove spines\n",
    "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "    \n",
    "    # Add a background\n",
    "    plt.gca().set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select top 5 features based on importance\n",
    "    top_features = importance.nlargest(min(5, len(importance))).index.tolist()\n",
    "    print(f\"Top 5 features: {top_features}\")\n",
    "    \n",
    "    # Compare performance with and without feature selection\n",
    "    print(\"\\n4. Model Performance Comparison:\")\n",
    "    \n",
    "    # Baseline model (all features)\n",
    "    model_all = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    model_all.fit(X_train_res, y_train_res)\n",
    "    y_pred_all = model_all.predict(X_test)\n",
    "    accuracy_all = model_all.score(X_test, y_test)\n",
    "    print(\"All features accuracy:\", accuracy_all)\n",
    "    \n",
    "    # Model with top 5 features from RF importance\n",
    "    top_feature_codes = [feat for feat in selected_features if get_feature_name(feat) in top_features]\n",
    "    X_train_top = X_train_res[top_feature_codes]\n",
    "    X_test_top = X_test[top_feature_codes]\n",
    "    \n",
    "    model_top = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    model_top.fit(X_train_top, y_train_res)\n",
    "    y_pred_top = model_top.predict(X_test_top)\n",
    "    accuracy_top = model_top.score(X_test_top, y_test)\n",
    "    print(\"Top 5 features accuracy:\", accuracy_top)\n",
    "    \n",
    "    # Model with SelectKBest features\n",
    "    X_train_kbest = X_train_res[selected_features_kbest]\n",
    "    X_test_kbest = X_test[selected_features_kbest]\n",
    "    \n",
    "    model_kbest = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    model_kbest.fit(X_train_kbest, y_train_res)\n",
    "    y_pred_kbest = model_kbest.predict(X_test_kbest)\n",
    "    accuracy_kbest = model_kbest.score(X_test_kbest, y_test)\n",
    "    print(\"SelectKBest features accuracy:\", accuracy_kbest)\n",
    "    \n",
    "    # Model with RFE features\n",
    "    X_train_rfe = X_train_res[selected_features_rfe]\n",
    "    X_test_rfe = X_test[selected_features_rfe]\n",
    "    \n",
    "    model_rfe = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    model_rfe.fit(X_train_rfe, y_train_res)\n",
    "    y_pred_rfe = model_rfe.predict(X_test_rfe)\n",
    "    accuracy_rfe = model_rfe.score(X_test_rfe, y_test)\n",
    "    print(\"RFE features accuracy:\", accuracy_rfe)\n",
    "    \n",
    "    # Create a comparison chart\n",
    "    methods = ['All Features', 'Top 5 Features', 'SelectKBest', 'RFE']\n",
    "    accuracies = [accuracy_all, accuracy_top, accuracy_kbest, accuracy_rfe]\n",
    "    \n",
    "    # Create a comparison bar chart\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    bars = plt.bar(methods, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Styling\n",
    "    plt.ylabel('Accuracy', fontweight='bold', fontsize=12)\n",
    "    plt.title('Model Accuracy', fontsize=12, fontweight='bold', pad=20)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Remove spines\n",
    "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "    \n",
    "    # Add a background\n",
    "    plt.gca().set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare classification reports\n",
    "    print(\"\\nClassification Report - All Features:\")\n",
    "    print(classification_report(y_test, y_pred_all))\n",
    "    \n",
    "    print(\"Classification Report - Top 5 Features:\")\n",
    "    print(classification_report(y_test, y_pred_top))\n",
    "    \n",
    "    # Create confusion matrix for the best model\n",
    "    best_model = model_top  # Using top features model\n",
    "    y_pred_best = best_model.predict(X_test_top)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_best)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Employed', 'Unemployed'],\n",
    "                yticklabels=['Employed', 'Unemployed'])\n",
    "    plt.title('Confusion Matrix - Employment Prediction', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.ylabel('Actual', fontweight='bold')\n",
    "    plt.xlabel('Predicted', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Precision-Recall curve for the minority class\n",
    "    y_prob = best_model.predict_proba(X_test_top)[:, 1]  # Probability for class 2 (unemployed)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test-1, y_prob)  # Convert to 0/1 for sklearn\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(recall, precision, marker='.', label='Random Forest')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve for Minority Class (Unemployed)', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Calculate AUC\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f\"Precision-Recall AUC: {pr_auc:.3f}\")\n",
    "    \n",
    "    # Find optimal threshold (maximizing F1-score)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_precision = precision[optimal_idx]\n",
    "    optimal_recall = recall[optimal_idx]\n",
    "    \n",
    "    plt.plot(optimal_recall, optimal_precision, 'ro', markersize=10, \n",
    "             label=f'Optimal Threshold ({optimal_threshold:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"Optimal Precision: {optimal_precision:.3f}\")\n",
    "    print(f\"Optimal Recall: {optimal_recall:.3f}\")\n",
    "    \n",
    "    # Apply optimal threshold\n",
    "    y_pred_optimal = (y_prob >= optimal_threshold).astype(int) + 1  # Convert back to 1/2\n",
    "    \n",
    "    print(\"\\nClassification Report with Optimal Threshold:\")\n",
    "    print(classification_report(y_test, y_pred_optimal))\n",
    "    \n",
    "    # Add XGBoost model after the Random Forest models\n",
    "    print(\"\\n6. XGBoost Model with Class Weighting:\")\n",
    "    \n",
    "    # Convert labels from [1, 2] to [0, 1] for XGBoost\n",
    "    y_train_xgb = y_train_res - 1\n",
    "    y_test_xgb = y_test - 1\n",
    "    \n",
    "    # Calculate the ratio for scale_pos_weight\n",
    "    count_majority = (y_train == 1).sum()\n",
    "    count_minority = (y_train == 2).sum()\n",
    "    ratio = count_majority / count_minority\n",
    "    print(f\"Class ratio (majority:minority): {ratio:.2f}:1\")\n",
    "    \n",
    "    # XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        random_state=42,\n",
    "        scale_pos_weight=ratio,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train_res[top_feature_codes], y_train_xgb)\n",
    "    y_pred_xgb = xgb_model.predict(X_test_top)\n",
    "    \n",
    "    # Convert predictions back to original labels [1, 2]\n",
    "    y_pred_xgb_original = y_pred_xgb + 1\n",
    "    \n",
    "    print(\"XGBoost Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_xgb_original))\n",
    "    \n",
    "    # Plot XGBoost feature importance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    xgb_importance = pd.Series(xgb_model.feature_importances_, index=top_feature_codes)\n",
    "    xgb_importance.index = [get_feature_name(feat) for feat in xgb_importance.index]\n",
    "    xgb_importance = xgb_importance.sort_values(ascending=True)\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(xgb_importance)))\n",
    "    bars = plt.barh(xgb_importance.index, xgb_importance.values, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value annotations on bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Importance Score', fontweight='bold', fontsize=12)\n",
    "    plt.ylabel('Features', fontweight='bold', fontsize=12)\n",
    "    plt.title('XGBoost Feature Importance', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "    plt.gca().set_facecolor('#f8f9fa')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Cross-validation to validate results\n",
    "    print(\"\\n7. Cross-Validation Results:\")\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model_top, X_train_res[top_feature_codes], y_train_res, \n",
    "                               cv=cv, scoring='f1_weighted')\n",
    "    print(f\"Cross-Validation F1 Scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Run predictive modeling with the fixed function\n",
    "predict_employment(df, potential_features)\n",
    "\n",
    "# Additional analysis: Employment outcomes by program\n",
    "def employment_by_program(df):\n",
    "    print(\"\\n=== Employment Outcomes by Program ===\\n\")\n",
    "    \n",
    "    if 'PGMCIPAP' not in df.columns or 'LFSTATP' not in df.columns:\n",
    "        print(\"Program or employment data not available.\")\n",
    "        return\n",
    "    \n",
    "    # Create cross-tabulation of program vs employment status\n",
    "    program_emp = pd.crosstab(df['PGMCIPAP'], df['LFSTATP'])\n",
    "    \n",
    "    # Filter for programs with sufficient data\n",
    "    program_emp = program_emp[program_emp.sum(axis=1) > 50]\n",
    "    \n",
    "    if len(program_emp) == 0:\n",
    "        print(\"Insufficient data for program employment analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate employment rate (employed / (employed + unemployed))\n",
    "    program_emp['Employment_Rate'] = program_emp[1] / (program_emp[1] + program_emp[2]) * 100\n",
    "    \n",
    "    # Sort by employment rate\n",
    "    program_emp = program_emp.sort_values('Employment_Rate', ascending=False)\n",
    "    \n",
    "    # Get all programs (not just top/bottom 10 since we have only 5)\n",
    "    all_programs = program_emp.copy()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Get program names - convert program codes to strings for lookup\n",
    "    program_labels = []\n",
    "    for pid in all_programs.index:\n",
    "        # Convert program code to string for lookup\n",
    "        program_code_str = str(int(pid)) if pid.is_integer() else str(pid)\n",
    "        program_name = get_response_value('PGMCIPAP', program_code_str)\n",
    "        if program_name == program_code_str:  # If no description found, use generic name\n",
    "            program_name = f\"Program {pid}\"\n",
    "        program_labels.append(program_name)\n",
    "    \n",
    "    # Create bar chart\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(all_programs)))\n",
    "    bars = ax.barh(range(len(all_programs)), all_programs['Employment_Rate'], \n",
    "                   color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_yticks(range(len(all_programs)))\n",
    "    ax.set_yticklabels(program_labels, fontsize=18)\n",
    "    ax.set_xlabel('Employment Rate (%)', fontweight='bold')\n",
    "    ax.set_title('Employment Rate by Program', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.invert_yaxis()  # Highest employment at the top\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Remove spines\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results with more information\n",
    "    print(\"Programs by Employment Rate (with sufficient data):\")\n",
    "    for pid, row in all_programs.iterrows():\n",
    "        # Convert program code to string for lookup\n",
    "        program_code_str = str(int(pid)) if pid.is_integer() else str(pid)\n",
    "        program_name = get_response_value('PGMCIPAP', program_code_str)\n",
    "        if program_name == program_code_str:  # If no description found, use generic name\n",
    "            program_name = f\"Program {pid}\"\n",
    "        \n",
    "        # Use .iloc to avoid the warning about position-based indexing\n",
    "        employed = row.iloc[0]  # First column (employed)\n",
    "        unemployed = row.iloc[1]  # Second column (unemployed)\n",
    "        total = employed + unemployed\n",
    "        rate = row['Employment_Rate']\n",
    "        \n",
    "        print(f\"{program_name}: {rate:.1f}% ({employed}/{total} employed)\")\n",
    "    \n",
    "    # Provide additional context\n",
    "    print(f\"\\nNote: Only {len(all_programs)} programs had sufficient data (>50 respondents)\")\n",
    "\n",
    "# Run the program analysis\n",
    "employment_by_program(df)\n",
    "\n",
    "# Employment Income Heatmap by Region and Program with Mapped Labels\n",
    "def employment_income_heatmap(df):\n",
    "    print(\"\\n=== Employment Income Heatmap (JOBINCP) ===\\n\")\n",
    "    \n",
    "    # Check necessary columns\n",
    "    if 'JOBINCP' not in df.columns or 'REG_RESP' not in df.columns or 'PGMCIPAP' not in df.columns:\n",
    "        print(\"Required columns for income heatmap not available.\")\n",
    "        return\n",
    "    \n",
    "    # Filter for employed individuals only\n",
    "    employed_df = df[df['LFSTATP'] == 1].copy()\n",
    "    \n",
    "    # Map JOBINCP codes to midpoint values\n",
    "    income_mapping = {\n",
    "        1: 15000,   # Under $30000 (midpoint)\n",
    "        2: 40000,   # $30000-$49999\n",
    "        3: 60000,   # $50000-$69999\n",
    "        4: 80000,   # $70000-$89999\n",
    "        5: 95000    # $90000+ (approximate midpoint)\n",
    "    }\n",
    "    \n",
    "    # Map region codes to region names\n",
    "    region_mapping = {\n",
    "        1: \"Atlantic provinces\",\n",
    "        2: \"Quebec\",\n",
    "        3: \"Ontario\",\n",
    "        4: \"Western provinces territories\"\n",
    "    }\n",
    "    \n",
    "    # Map program codes to program names\n",
    "    program_mapping = {\n",
    "        1: \"Education\",\n",
    "        4: \"Social/behavioral sciences/law\",\n",
    "        5: \"Business/management/public admin\",\n",
    "        6: \"Physical/life sciences/technologies\",\n",
    "        7: \"Math/computer/info sciences\",\n",
    "        8: \"Architecture/engineering/trades\",\n",
    "        9: \"Health fields\",\n",
    "        10: \"Other\"\n",
    "    }\n",
    "    \n",
    "    employed_df['INCOME_MIDPOINT'] = employed_df['JOBINCP'].map(income_mapping)\n",
    "    employed_df['REGION_NAME'] = employed_df['REG_RESP'].map(region_mapping)\n",
    "    employed_df['PROGRAM_NAME'] = employed_df['PGMCIPAP'].map(program_mapping)\n",
    "    \n",
    "    # Remove rows with missing income, region, or program data\n",
    "    employed_df = employed_df.dropna(subset=['INCOME_MIDPOINT', 'REGION_NAME', 'PROGRAM_NAME'])\n",
    "    \n",
    "    # Group by region and program, calculate mean income\n",
    "    income_by_region_program = employed_df.groupby(['REGION_NAME', 'PROGRAM_NAME'])['INCOME_MIDPOINT'].mean().unstack()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    ax = sns.heatmap(\n",
    "        income_by_region_program,\n",
    "        annot=True,\n",
    "        fmt=\".0f\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        cbar_kws={'label': 'Average Income ($)'},\n",
    "        linewidths=0.5,\n",
    "        linecolor='grey'\n",
    "    )\n",
    "    \n",
    "    # Set colorbar label size\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.set_ylabel('Average Income ($)', fontsize=20)\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    plt.title('Average Employment Income by Region and Program\\n(JOBINCP Midpoint Values)', \n",
    "              fontsize=24, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Program', fontweight='bold', fontsize=20)\n",
    "    plt.ylabel('Region', fontweight='bold', fontsize=20)\n",
    "    \n",
    "    # Set tick label size to 20\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=20)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=20)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=45, ha='right')    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some summary statistics\n",
    "    print(\"Summary Statistics:\")\n",
    "    print(f\"Total employed individuals in analysis: {len(employed_df)}\")\n",
    "    print(f\"Average income across all employed: ${employed_df['INCOME_MIDPOINT'].mean():.0f}\")\n",
    "    print(f\"Highest paying region-program combination: ${income_by_region_program.max().max():.0f}\")\n",
    "    print(f\"Lowest paying region-program combination: ${income_by_region_program.min().min():.0f}\")\n",
    "\n",
    "# Call the function\n",
    "employment_income_heatmap(df)\n",
    "\n",
    "\n",
    "###########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb24c80-b2ab-4c7d-906c-62c50b966688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "recalls = [0.22, 0.31, 0.6]  # Changed variable name to reflect metric\n",
    "model_names = ['Random Forest', 'RF with Threshold', 'XGBoost']\n",
    "\n",
    "plt.figure(figsize=(3.8, 3))  # Increased figure size for better readability\n",
    "bars = plt.bar(model_names, recalls, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Unemployed Class Recall', fontsize=9)\n",
    "plt.ylabel('Recall', fontsize=8)  # Changed y-axis label to match metric\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bar, recall in zip(bars, recalls):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{recall:.2f}', ha='center', va='bottom', fontsize=10)  # Consistent font size\n",
    "\n",
    "plt.xticks(rotation=10, fontsize=9)\n",
    "plt.yticks(fontsize=9)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201d026-263c-4028-bc56-9cd503f125ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
